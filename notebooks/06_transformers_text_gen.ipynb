{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6efd990",
   "metadata": {},
   "source": [
    "# GENERACIÓN DE TEXTO USANDO TRANSFORMERS\n",
    "\n",
    "Modelo: EleutherAI/gpt-neo-125M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f339fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3442ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Describe una imagen de una ciudad futurista al atardecer.\",\n",
    "    \"Inventa una historia de ciencia ficción en 3 líneas.\",\n",
    "    \"Escribe una descripción poética de un bosque encantado.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51aa3e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7999660813c410db17d5a014285eb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\calag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\calag\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-125M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1b6c56af544a3fbff90e603da9bff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172a3fd3252f4c5084550056963f0e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d6c016e7cf4951aa71b181953e5dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd752e77da7e40d69bf1a4c2bc0c9e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abff2b5b2bea4b5098266887a6a0b40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2702f08ae24ad7964a08f01c5e265d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e3b481b8cd4d6e83a0a5f0335d68c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125M\"  # Más potente que GPT-2\n",
    "\n",
    "# Cargar tokenizer y modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Crear pipeline de generación\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43666371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para prompt: Describe una imagen de una ciudad futurista al atardecer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Describe una imagen de una ciudad futurista al atardecer. El desplome, y esta imagen está construida por las fuerzas de los ataques de ataque a la mujer; habida cuenta del establecimiento de su cuerpo. No se preocupa si la imagen está fija en la cita de la mano de su mujer, si para ciudad y medio, el esplendor se lo lleva al uno.\n",
      "\n",
      "La mujer entrena en el cielo de su mano, levanta la mano\n",
      "\n",
      "Generando para prompt: Inventa una historia de ciencia ficción en 3 líneas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Inventa una historia de ciencia ficción en 3 líneas. Funtionadas fuentes al tema.\n",
      "\n",
      "Lo estado eso es que al menos unos 2% de la riqueza mundial a la luz se han dejado el ejercicio de la vida.\n",
      "\n",
      "Los cientos de los más grandes ricos han podido olvidar si la vida estaba disponible en el centro de sus propiedades a la luz. La pobreza del pueblo hace 2 años, alcanzaron la vida a la esquina del\n",
      "\n",
      "Generando para prompt: Escribe una descripción poética de un bosque encantado.\n",
      ">>> Escribe una descripción poética de un bosque encantado.\n",
      "\n",
      "Aplicación en #sensasinhos\n",
      "\n",
      "La estructura de la sociedad humana hace unas cuestiones de la capacidad de vida y la capacidad de vida social. Las técnicas de sus códices, el estado de la sociedad humana, se establece una estructura para hacer ver las aprobaciones de su proyecto social. Su estructura hace que también pueda tener un cambio de la estructura de los c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Generando para prompt: {prompt}\")\n",
    "    output = generator(prompt, max_length=150, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)\n",
    "    text = output[0][\"generated_text\"]\n",
    "    print(f\">>> {text}\\n\")\n",
    "    responses.append({\"prompt\": prompt, \"response\": text.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e88a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generaciones guardadas en generations/transformers_outputs.json\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"generations\", exist_ok=True)\n",
    "with open(\"generations/transformers_outputs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Generaciones guardadas en generations/transformers_outputs.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
