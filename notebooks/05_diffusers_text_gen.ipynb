{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efad1727",
   "metadata": {},
   "source": [
    "# GENERACIÓN DE TEXTO USANDO \"DIFFUSERS\"\n",
    "\n",
    "Realmente no lo estamosa usando, se usa transformers por compatibilidad\n",
    "\n",
    "MODELOS: Prueba con \"gpt2\"\n",
    "         Prueba con \"tiiuae/falcon-7b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774ba21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4631933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Describe una imagen de una ciudad futurista al atardecer.\",\n",
    "    \"Inventa una historia de ciencia ficción en 3 líneas.\",\n",
    "    \"Escribe una descripción poética de un bosque encantado.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUEDO CAMBIAR EL MODELO POR UNO MÁS POTENTE (VER INICIO DEL CUADERNO)\n",
    "model_name = \"gpt2\"  # LIGERO Y NO NECESITA MI AUTENTICACIÍON EN Hugging Face\n",
    "\n",
    "# CARGAMOS TOKENIZER Y MODELO\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "#CREAMOS PIPELINE PARA GENERACIÓN\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7189dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando para prompt: Describe una imagen de una ciudad futurista al atardecer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Describe una imagen de una ciudad futurista al atardecer. A parte dans la sue de liefer de los mórsa del roger; nach un parte ciudad, la parte de los mórsa del roger. Pare la parte de los mórsa del roger. A parte dans la sue de liefer de los mórsa del roger. pore un parte del roger. Una a parte de los mórsa del roger. Una a parte del roger. (The above text does not constitute a full representation of what the\n",
      "\n",
      "Generando para prompt: Inventa una historia de ciencia ficción en 3 líneas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Inventa una historia de ciencia ficción en 3 líneas.\n",
      "\n",
      "2.2 The history of the Aztecs\n",
      "\n",
      "Cited from a book, Historia en Azteca, by E.H.C. Spero, ed., PAGES 38–43.\n",
      "\n",
      "3. Aztec history of the west and east\n",
      "\n",
      "Spero, E.H.C. Spero. The Aztecs, p. 642\n",
      "\n",
      "4. Aztec myth with historical references and geography\n",
      "\n",
      "Pallo, R.J. Dalla Lana, p. 1090\n",
      "\n",
      "5. Aztec civilization in Central America and its impact on the\n",
      "\n",
      "Generando para prompt: Escribe una descripción poética de un bosque encantado.\n",
      ">>> Escribe una descripción poética de un bosque encantado.\n",
      "\n",
      "Sándor Pérez día périas la libertad en México.\n",
      "\n",
      "La serización de los ganados de la suas y pídas de la compédida, en la vida o en este español en la sua, en la vida que empresario del nuestro en el esto, se vuotó la quiero.\n",
      "\n",
      "La una más a nuestro hacia para es sua en la sua más a sua.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Generando para prompt: {prompt}\")\n",
    "    output = generator(prompt, max_length=150, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)\n",
    "    text = output[0][\"generated_text\"]\n",
    "    print(f\">>> {text}\\n\")\n",
    "    responses.append({\"prompt\": prompt, \"response\": text.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7371122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generaciones guardadas en generations/diffusers_outputs.json\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"generations\", exist_ok=True)\n",
    "with open(\"generations/diffusers_outputs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Generaciones guardadas en generations/diffusers_outputs.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
